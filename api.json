[
    {
        "id": "234",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.713147",
        "title": "Support Vector Machines",
        "description": "Sklearn Support Vector Machines (SVM)",
        "model_type": "hyperplane",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.HelmertEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BaseNEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", SVC())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    PowerTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.HelmertEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), StandardScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.BaseNEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", SVC()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8230337078651685,\n        \"precision\": 0.8174405355362484,\n        \"recall\": 0.7996840123705795,\n        \"f1\": 0.8065217391304348,\n        \"balancedAccuracyScore\": 0.7996840123705795,\n        \"auc_roc\": 0.7996840123705796,\n        \"log_loss\": 6.378511695265114,\n        \"jaccard_score\": 0.6,\n        \"hinge_loss\": 0.800561797752809,\n        \"hamming_loss\": 0.17696629213483145,\n        \"avgPreScore\": 0.6757318080138254\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.805623973727422,\n        \"recall\": 0.7874517374517374,\n        \"f1\": 0.7930772533606367,\n        \"balancedAccuracyScore\": 0.7874517374517374,\n        \"auc_roc\": 0.7874517374517374,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.5930232558139535,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6864066780267898\n    }\n}",
        "accuracy": "0.823",
        "precision": "0.8174",
        "recall": "0.7997",
        "f1": "0.8065",
        "holdout": "6.3785",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8230337078651685
    },
    {
        "id": "256",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:30.070993",
        "title": "Random Forest Classifier",
        "description": "Python Sklearn Random Forest Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.BackwardDifferenceEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.HelmertEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", RandomForestClassifier(random_state=42))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    FunctionTransformer(np.log1p),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.BackwardDifferenceEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.HelmertEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", RandomForestClassifier(random_state=42)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8216292134831461,\n        \"precision\": 0.8173583173583174,\n        \"recall\": 0.7963392496974586,\n        \"f1\": 0.8040968423333152,\n        \"balancedAccuracyScore\": 0.7963392496974586,\n        \"auc_roc\": 0.7963392496974586,\n        \"log_loss\": 6.4291348039576945,\n        \"jaccard_score\": 0.5942492012779552,\n        \"hinge_loss\": 0.8019662921348315,\n        \"hamming_loss\": 0.17837078651685392,\n        \"avgPreScore\": 0.6739977697968642\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.8081411503195333,\n        \"recall\": 0.7854568854568855,\n        \"f1\": 0.7918950340475005,\n        \"balancedAccuracyScore\": 0.7854568854568855,\n        \"auc_roc\": 0.7854568854568855,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.5882352941176471,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6879107333361386\n    }\n}",
        "accuracy": "0.8216",
        "precision": "0.8174",
        "recall": "0.7963",
        "f1": "0.8041",
        "holdout": "6.4291",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8216292134831461
    },
    {
        "id": "241",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.695669",
        "title": "Random Forest Classifier",
        "description": "Python Sklearn Random Forest Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.SumEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.MEstimateEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", RandomForestClassifier(random_state=42))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.SumEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.MEstimateEncoder()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", RandomForestClassifier(random_state=42)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "7",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8174157303370787,\n        \"precision\": 0.8079134199134199,\n        \"recall\": 0.7988772354443996,\n        \"f1\": 0.8027734451575511,\n        \"balancedAccuracyScore\": 0.7988772354443996,\n        \"auc_roc\": 0.7988772354443996,\n        \"log_loss\": 6.581004130035435,\n        \"jaccard_score\": 0.5987654320987654,\n        \"hinge_loss\": 0.8061797752808989,\n        \"hamming_loss\": 0.18258426966292135,\n        \"avgPreScore\": 0.665663927553245\n    },\n    \"holdout\": {\n        \"accuracy\": 0.776536312849162,\n        \"precision\": 0.7782212885154062,\n        \"recall\": 0.7556628056628056,\n        \"f1\": 0.7614605543710022,\n        \"balancedAccuracyScore\": 0.7556628056628056,\n        \"auc_roc\": 0.7556628056628056,\n        \"log_loss\": 8.054447684718916,\n        \"jaccard_score\": 0.5402298850574713,\n        \"hinge_loss\": 0.8100558659217877,\n        \"hamming_loss\": 0.22346368715083798,\n        \"avgPreScore\": 0.6483605113493381\n    }\n}",
        "accuracy": "0.8174",
        "precision": "0.8079",
        "recall": "0.7989",
        "f1": "0.8028",
        "holdout": "6.581",
        "cv_score": "8.0544",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8174157303370787
    },
    {
        "id": "249",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.629049",
        "title": "Ridge Regressor",
        "description": "Sklearn Ridge Regressor Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.BaseNEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.CountEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), StandardScaler()),\n\t)),\n\t(\"clf\", RidgeClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    RobustScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.BaseNEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.CountEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), StandardScaler()),\n            ),\n        ),\n        (\"clf\", RidgeClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8146067415730337,\n        \"precision\": 0.8123368266225409,\n        \"recall\": 0.7855318004571736,\n        \"f1\": 0.7946423108858105,\n        \"balancedAccuracyScore\": 0.7855318004571736,\n        \"auc_roc\": 0.7855318004571735,\n        \"log_loss\": 6.682250347420596,\n        \"jaccard_score\": 0.5755627009646302,\n        \"hinge_loss\": 0.8089887640449438,\n        \"hamming_loss\": 0.1853932584269663,\n        \"avgPreScore\": 0.6635404060777195\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7597765363128491,\n        \"precision\": 0.7773940345368917,\n        \"recall\": 0.7274131274131275,\n        \"f1\": 0.7337138903303927,\n        \"balancedAccuracyScore\": 0.7274131274131275,\n        \"auc_roc\": 0.7274131274131274,\n        \"log_loss\": 8.658531261072836,\n        \"jaccard_score\": 0.4819277108433735,\n        \"hinge_loss\": 0.8268156424581006,\n        \"hamming_loss\": 0.24022346368715083,\n        \"avgPreScore\": 0.6312017181929392\n    }\n}",
        "accuracy": "0.8146",
        "precision": "0.8123",
        "recall": "0.7855",
        "f1": "0.7946",
        "holdout": "6.6823",
        "cv_score": "8.6585",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8146067415730337
    },
    {
        "id": "244",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.995829",
        "title": "Support Vector Machines",
        "description": "Sklearn Support Vector Machines (SVM)",
        "model_type": "hyperplane",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-999)),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), OneHotEncoder(handle_unknown='ignore')),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", SVC())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    RobustScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-999\n                    ),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), PowerTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]), OneHotEncoder(handle_unknown=\"ignore\")\n                ),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", SVC()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "11",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8132022471910112,\n        \"precision\": 0.806089273817455,\n        \"recall\": 0.7895824929407018,\n        \"f1\": 0.7959555694416014,\n        \"balancedAccuracyScore\": 0.7895824929407018,\n        \"auc_roc\": 0.7895824929407018,\n        \"log_loss\": 6.732873456113175,\n        \"jaccard_score\": 0.5830721003134797,\n        \"hinge_loss\": 0.8103932584269663,\n        \"hamming_loss\": 0.18679775280898878,\n        \"avgPreScore\": 0.6598501943419016\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.8035087719298246,\n        \"recall\": 0.7894465894465894,\n        \"f1\": 0.7941920436253737,\n        \"balancedAccuracyScore\": 0.7894465894465894,\n        \"auc_roc\": 0.7894465894465894,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.5977011494252874,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6850671900951231\n    }\n}",
        "accuracy": "0.8132",
        "precision": "0.8061",
        "recall": "0.7896",
        "f1": "0.796",
        "holdout": "6.7329",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8132022471910112
    },
    {
        "id": "253",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.838612",
        "title": "Linear Support Vector Machines",
        "description": "Sklearn Linear Support Vector Machines (SVM) Classifier",
        "model_type": "hyperplane",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.BackwardDifferenceEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.LeaveOneOutEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", LinearSVC())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    MinMaxScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.BackwardDifferenceEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), StandardScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.LeaveOneOutEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", LinearSVC()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "10",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8089887640449438,\n        \"precision\": 0.797765899122807,\n        \"recall\": 0.7921204786876428,\n        \"f1\": 0.7946734520780322,\n        \"balancedAccuracyScore\": 0.7921204786876428,\n        \"auc_roc\": 0.7921204786876428,\n        \"log_loss\": 6.884742782190917,\n        \"jaccard_score\": 0.5878787878787879,\n        \"hinge_loss\": 0.8146067415730337,\n        \"hamming_loss\": 0.19101123595505617,\n        \"avgPreScore\": 0.652498349195036\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8324022346368715,\n        \"precision\": 0.8349184782608696,\n        \"recall\": 0.8172458172458172,\n        \"f1\": 0.8231225296442688,\n        \"balancedAccuracyScore\": 0.8172458172458172,\n        \"auc_roc\": 0.8172458172458174,\n        \"log_loss\": 6.040835763539188,\n        \"jaccard_score\": 0.6428571428571429,\n        \"hinge_loss\": 0.7541899441340782,\n        \"hamming_loss\": 0.16759776536312848,\n        \"avgPreScore\": 0.7274413030348784\n    }\n}",
        "accuracy": "0.809",
        "precision": "0.7978",
        "recall": "0.7921",
        "f1": "0.7947",
        "holdout": "6.8847",
        "cv_score": "6.0408",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8089887640449438
    },
    {
        "id": "248",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.642440",
        "title": "Gradient Boosting Classifier",
        "description": "Sklearn Gradient Boosting Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.GLMMEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-999)),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", GradientBoostingClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    PowerTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.GLMMEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), MinMaxScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]),\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-999\n                    ),\n                ),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", GradientBoostingClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8075842696629213,\n        \"precision\": 0.7980203644627015,\n        \"recall\": 0.7865570794675272,\n        \"f1\": 0.7912796382956416,\n        \"balancedAccuracyScore\": 0.7865570794675272,\n        \"auc_roc\": 0.7865570794675273,\n        \"log_loss\": 6.9353658908834985,\n        \"jaccard_score\": 0.5784615384615385,\n        \"hinge_loss\": 0.8160112359550562,\n        \"hamming_loss\": 0.19241573033707865,\n        \"avgPreScore\": 0.6506477016431258\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.8035087719298246,\n        \"recall\": 0.7894465894465894,\n        \"f1\": 0.7941920436253737,\n        \"balancedAccuracyScore\": 0.7894465894465894,\n        \"auc_roc\": 0.7894465894465894,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.5977011494252874,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6850671900951231\n    }\n}",
        "accuracy": "0.8076",
        "precision": "0.798",
        "recall": "0.7866",
        "f1": "0.7913",
        "holdout": "6.9354",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8075842696629213
    },
    {
        "id": "242",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.802112",
        "title": "Linear Support Vector Machines",
        "description": "Sklearn Linear Support Vector Machines (SVM) Classifier",
        "model_type": "hyperplane",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.TargetEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.JamesSteinEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), RobustScaler()),\n\t)),\n\t(\"clf\", LinearSVC())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.TargetEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), MinMaxScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.JamesSteinEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), RobustScaler()),\n            ),\n        ),\n        (\"clf\", LinearSVC()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "10",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8061797752808989,\n        \"precision\": 0.7974929378531074,\n        \"recall\": 0.7832123167944064,\n        \"f1\": 0.7888457174294261,\n        \"balancedAccuracyScore\": 0.7832123167944064,\n        \"auc_roc\": 0.7832123167944064,\n        \"log_loss\": 6.985988999576077,\n        \"jaccard_score\": 0.5727554179566563,\n        \"hinge_loss\": 0.8174157303370787,\n        \"hamming_loss\": 0.19382022471910113,\n        \"avgPreScore\": 0.6486781332103528\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7821229050279329,\n        \"precision\": 0.7960467205750225,\n        \"recall\": 0.7544401544401544,\n        \"f1\": 0.7620411084978015,\n        \"balancedAccuracyScore\": 0.7544401544401544,\n        \"auc_roc\": 0.7544401544401544,\n        \"log_loss\": 7.853086492600944,\n        \"jaccard_score\": 0.5301204819277109,\n        \"hinge_loss\": 0.8044692737430168,\n        \"hamming_loss\": 0.21787709497206703,\n        \"avgPreScore\": 0.6612234665359995\n    }\n}",
        "accuracy": "0.8062",
        "precision": "0.7975",
        "recall": "0.7832",
        "f1": "0.7888",
        "holdout": "6.986",
        "cv_score": "7.8531",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8061797752808989
    },
    {
        "id": "230",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.814893",
        "title": "Random Forest Classifier",
        "description": "Python Sklearn Random Forest Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.WOEEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", RandomForestClassifier(random_state=42))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), PowerTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.WOEEncoder()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", RandomForestClassifier(random_state=42)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8047752808988764,\n        \"precision\": 0.7992869794268587,\n        \"recall\": 0.7769093720586258,\n        \"f1\": 0.7847843341922998,\n        \"balancedAccuracyScore\": 0.7769093720586258,\n        \"auc_roc\": 0.7769093720586258,\n        \"log_loss\": 7.036612108268659,\n        \"jaccard_score\": 0.5615141955835962,\n        \"hinge_loss\": 0.8188202247191011,\n        \"hamming_loss\": 0.1952247191011236,\n        \"avgPreScore\": 0.6472145410649075\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7877094972067039,\n        \"precision\": 0.7936021658592192,\n        \"recall\": 0.7651866151866151,\n        \"f1\": 0.771989809600429,\n        \"balancedAccuracyScore\": 0.7651866151866151,\n        \"auc_roc\": 0.7651866151866152,\n        \"log_loss\": 7.651725300482972,\n        \"jaccard_score\": 0.5529411764705883,\n        \"hinge_loss\": 0.7988826815642458,\n        \"hamming_loss\": 0.2122905027932961,\n        \"avgPreScore\": 0.6655164604018389\n    }\n}",
        "accuracy": "0.8048",
        "precision": "0.7993",
        "recall": "0.7769",
        "f1": "0.7848",
        "holdout": "7.0366",
        "cv_score": "7.6517",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8047752808988764
    },
    {
        "id": "231",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.051707",
        "title": "Bagging using Decision Tree Classifier",
        "description": "Sklearn Bagging using Decision Tree Classifier",
        "model_type": "tree",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), OneHotEncoder(handle_unknown='ignore')),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.MEstimateEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", BaggingClassifier(base_estimator=base_classifier))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    OneHotEncoder(handle_unknown=\"ignore\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), Normalizer()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.MEstimateEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", BaggingClassifier(base_estimator=base_classifier)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "10",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.8033707865168539,\n        \"precision\": 0.8029433962264152,\n        \"recall\": 0.7698668818071803,\n        \"f1\": 0.7800141242937854,\n        \"balancedAccuracyScore\": 0.7698668818071803,\n        \"auc_roc\": 0.7698668818071803,\n        \"log_loss\": 7.087235216961238,\n        \"jaccard_score\": 0.5483870967741935,\n        \"hinge_loss\": 0.8202247191011236,\n        \"hamming_loss\": 0.19662921348314608,\n        \"avgPreScore\": 0.6462999819642513\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7821229050279329,\n        \"precision\": 0.8054945054945055,\n        \"recall\": 0.7504504504504504,\n        \"f1\": 0.758484691229891,\n        \"balancedAccuracyScore\": 0.7504504504504504,\n        \"auc_roc\": 0.7504504504504504,\n        \"log_loss\": 7.853086492600944,\n        \"jaccard_score\": 0.5185185185185185,\n        \"hinge_loss\": 0.8044692737430168,\n        \"hamming_loss\": 0.21787709497206703,\n        \"avgPreScore\": 0.6652574362071568\n    }\n}",
        "accuracy": "0.8034",
        "precision": "0.8029",
        "recall": "0.7699",
        "f1": "0.78",
        "holdout": "7.0872",
        "cv_score": "7.8531",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.8033707865168539
    },
    {
        "id": "225",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.901179",
        "title": "Gradient Boosting Classifier",
        "description": "Sklearn Gradient Boosting Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.JamesSteinEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.JamesSteinEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", GradientBoostingClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.JamesSteinEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.JamesSteinEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", GradientBoostingClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7991573033707865,\n        \"precision\": 0.7880290058723013,\n        \"recall\": 0.7790607771951055,\n        \"f1\": 0.7828708156419337,\n        \"balancedAccuracyScore\": 0.7790607771951055,\n        \"auc_roc\": 0.7790607771951055,\n        \"log_loss\": 7.239104543038978,\n        \"jaccard_score\": 0.5666666666666667,\n        \"hinge_loss\": 0.824438202247191,\n        \"hamming_loss\": 0.20084269662921347,\n        \"avgPreScore\": 0.6377855039140293\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.8017723880597014,\n        \"recall\": 0.7914414414414415,\n        \"f1\": 0.7952413635323725,\n        \"balancedAccuracyScore\": 0.7914414414414415,\n        \"auc_roc\": 0.7914414414414415,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.6022727272727273,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6838775321640327\n    }\n}",
        "accuracy": "0.7992",
        "precision": "0.788",
        "recall": "0.7791",
        "f1": "0.7829",
        "holdout": "7.2391",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7991573033707865
    },
    {
        "id": "235",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.442659",
        "title": "Ridge Regressor",
        "description": "Sklearn Ridge Regressor Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.MEstimateEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BinaryEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), Normalizer()),\n\t)),\n\t(\"clf\", RidgeClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    FunctionTransformer(np.log1p),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.MEstimateEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), RobustScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.BinaryEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), Normalizer()),\n            ),\n        ),\n        (\"clf\", RidgeClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.797752808988764,\n        \"precision\": 0.7866379310344828,\n        \"recall\": 0.7771951055533145,\n        \"f1\": 0.7811699620940478,\n        \"balancedAccuracyScore\": 0.7771951055533145,\n        \"auc_roc\": 0.7771951055533145,\n        \"log_loss\": 7.289727651731559,\n        \"jaccard_score\": 0.5636363636363636,\n        \"hinge_loss\": 0.8258426966292135,\n        \"hamming_loss\": 0.20224719101123595,\n        \"avgPreScore\": 0.6356909273855442\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7932960893854749,\n        \"precision\": 0.7914304993252361,\n        \"recall\": 0.7779279279279279,\n        \"f1\": 0.782431588975395,\n        \"balancedAccuracyScore\": 0.7779279279279279,\n        \"auc_roc\": 0.7779279279279279,\n        \"log_loss\": 7.450364108364997,\n        \"jaccard_score\": 0.5795454545454546,\n        \"hinge_loss\": 0.7932960893854749,\n        \"hamming_loss\": 0.20670391061452514,\n        \"avgPreScore\": 0.6692400608601726\n    }\n}",
        "accuracy": "0.7978",
        "precision": "0.7866",
        "recall": "0.7772",
        "f1": "0.7812",
        "holdout": "7.2897",
        "cv_score": "7.4504",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.797752808988764
    },
    {
        "id": "252",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.429985",
        "title": "AdaBoost Classifier",
        "description": "Sklearn AdaBoost Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-999)),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.WOEEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", AdaBoostClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    Normalizer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-999\n                    ),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), Normalizer()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.WOEEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", AdaBoostClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "10",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.797752808988764,\n        \"precision\": 0.7843476826653472,\n        \"recall\": 0.7897673793196182,\n        \"f1\": 0.7867110463906803,\n        \"balancedAccuracyScore\": 0.7897673793196182,\n        \"auc_roc\": 0.7897673793196182,\n        \"log_loss\": 7.289727651731559,\n        \"jaccard_score\": 0.5850144092219021,\n        \"hinge_loss\": 0.8258426966292135,\n        \"hamming_loss\": 0.20224719101123595,\n        \"avgPreScore\": 0.6365578276439938\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7877094972067039,\n        \"precision\": 0.8049666868564507,\n        \"recall\": 0.7592020592020592,\n        \"f1\": 0.7673097974822114,\n        \"balancedAccuracyScore\": 0.7592020592020592,\n        \"auc_roc\": 0.7592020592020593,\n        \"log_loss\": 7.651725300482971,\n        \"jaccard_score\": 0.5365853658536586,\n        \"hinge_loss\": 0.7988826815642458,\n        \"hamming_loss\": 0.2122905027932961,\n        \"avgPreScore\": 0.6707162684816317\n    }\n}",
        "accuracy": "0.7978",
        "precision": "0.7843",
        "recall": "0.7898",
        "f1": "0.7867",
        "holdout": "7.2897",
        "cv_score": "7.6517",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.797752808988764
    },
    {
        "id": "254",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.100379",
        "title": "Logistic Regression",
        "description": "Sklearn Logistic Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), OneHotEncoder(handle_unknown='ignore')),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.GLMMEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t)),\n\t(\"clf\", LogisticRegression())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    PowerTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    OneHotEncoder(handle_unknown=\"ignore\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), PowerTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.GLMMEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n            ),\n        ),\n        (\"clf\", LogisticRegression()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7935393258426966,\n        \"precision\": 0.7848387645936187,\n        \"recall\": 0.7671608175339518,\n        \"f1\": 0.7736617772039885,\n        \"balancedAccuracyScore\": 0.7671608175339518,\n        \"auc_roc\": 0.7671608175339518,\n        \"log_loss\": 7.441596977809299,\n        \"jaccard_score\": 0.5462962962962963,\n        \"hinge_loss\": 0.8300561797752809,\n        \"hamming_loss\": 0.20646067415730338,\n        \"avgPreScore\": 0.6295225240916986\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7821229050279329,\n        \"precision\": 0.783273131425396,\n        \"recall\": 0.7624195624195624,\n        \"f1\": 0.7681116093672147,\n        \"balancedAccuracyScore\": 0.7624195624195624,\n        \"auc_roc\": 0.7624195624195624,\n        \"log_loss\": 7.853086492600944,\n        \"jaccard_score\": 0.5517241379310345,\n        \"hinge_loss\": 0.8044692737430168,\n        \"hamming_loss\": 0.21787709497206703,\n        \"avgPreScore\": 0.6556634480437026\n    }\n}",
        "accuracy": "0.7935",
        "precision": "0.7848",
        "recall": "0.7672",
        "f1": "0.7737",
        "holdout": "7.4416",
        "cv_score": "7.8531",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7935393258426966
    },
    {
        "id": "246",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.963883",
        "title": "Logistic Regression",
        "description": "Sklearn Logistic Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.WOEEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.TargetEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", LogisticRegression())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    StandardScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.WOEEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), RobustScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.TargetEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", LogisticRegression()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "11",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7879213483146067,\n        \"precision\": 0.775947934352009,\n        \"recall\": 0.766354040607772,\n        \"f1\": 0.770339752421844,\n        \"balancedAccuracyScore\": 0.766354040607772,\n        \"auc_roc\": 0.766354040607772,\n        \"log_loss\": 7.64408941257962,\n        \"jaccard_score\": 0.5465465465465466,\n        \"hinge_loss\": 0.8356741573033708,\n        \"hamming_loss\": 0.21207865168539325,\n        \"avgPreScore\": 0.6211792898311517\n    },\n    \"holdout\": {\n        \"accuracy\": 0.8044692737430168,\n        \"precision\": 0.8110875706214689,\n        \"recall\": 0.7834620334620335,\n        \"f1\": 0.7906432748538011,\n        \"balancedAccuracyScore\": 0.7834620334620335,\n        \"auc_roc\": 0.7834620334620336,\n        \"log_loss\": 7.047641724129053,\n        \"jaccard_score\": 0.5833333333333334,\n        \"hinge_loss\": 0.7821229050279329,\n        \"hamming_loss\": 0.19553072625698323,\n        \"avgPreScore\": 0.6895960916886966\n    }\n}",
        "accuracy": "0.7879",
        "precision": "0.7759",
        "recall": "0.7664",
        "f1": "0.7703",
        "holdout": "7.6441",
        "cv_score": "7.0476",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7879213483146067
    },
    {
        "id": "243",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.082123",
        "title": "AdaBoost Classifier",
        "description": "Sklearn AdaBoost Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.SumEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.CatBoostEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), StandardScaler()),\n\t)),\n\t(\"clf\", AdaBoostClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    FunctionTransformer(np.log1p),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.SumEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), MinMaxScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.CatBoostEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), StandardScaler()),\n            ),\n        ),\n        (\"clf\", AdaBoostClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7865168539325843,\n        \"precision\": 0.7727949134199135,\n        \"recall\": 0.7711442786069652,\n        \"f1\": 0.7719410634208841,\n        \"balancedAccuracyScore\": 0.7711442786069652,\n        \"auc_roc\": 0.7711442786069653,\n        \"log_loss\": 7.694712521272202,\n        \"jaccard_score\": 0.5555555555555556,\n        \"hinge_loss\": 0.8370786516853933,\n        \"hamming_loss\": 0.21348314606741572,\n        \"avgPreScore\": 0.6197834880754552\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7877094972067039,\n        \"precision\": 0.7968931475029035,\n        \"recall\": 0.7631917631917632,\n        \"f1\": 0.7705128205128204,\n        \"balancedAccuracyScore\": 0.7631917631917632,\n        \"auc_roc\": 0.7631917631917632,\n        \"log_loss\": 7.651725300482971,\n        \"jaccard_score\": 0.5476190476190477,\n        \"hinge_loss\": 0.7988826815642458,\n        \"hamming_loss\": 0.2122905027932961,\n        \"avgPreScore\": 0.6670423416233471\n    }\n}",
        "accuracy": "0.7865",
        "precision": "0.7728",
        "recall": "0.7711",
        "f1": "0.7719",
        "holdout": "7.6947",
        "cv_score": "7.6517",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7865168539325843
    },
    {
        "id": "250",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.068994",
        "title": "Bagging using Decision Tree Classifier",
        "description": "Sklearn Bagging using Decision Tree Classifier",
        "model_type": "tree",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.CatBoostEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BackwardDifferenceEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", BaggingClassifier(base_estimator=base_classifier))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    StandardScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.CatBoostEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Parch\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), Normalizer()),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]), ce.BackwardDifferenceEncoder()\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", BaggingClassifier(base_estimator=base_classifier)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7865168539325843,\n        \"precision\": 0.7737750172532781,\n        \"recall\": 0.7667070055129757,\n        \"f1\": 0.7697753573859769,\n        \"balancedAccuracyScore\": 0.7667070055129757,\n        \"auc_roc\": 0.7667070055129757,\n        \"log_loss\": 7.694712521272202,\n        \"jaccard_score\": 0.5476190476190477,\n        \"hinge_loss\": 0.8370786516853933,\n        \"hamming_loss\": 0.21348314606741572,\n        \"avgPreScore\": 0.6192805368555829\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7597765363128491,\n        \"precision\": 0.7692422881102127,\n        \"recall\": 0.7314028314028314,\n        \"f1\": 0.7376350683437298,\n        \"balancedAccuracyScore\": 0.7314028314028314,\n        \"auc_roc\": 0.7314028314028315,\n        \"log_loss\": 8.658531261072836,\n        \"jaccard_score\": 0.49411764705882355,\n        \"hinge_loss\": 0.8268156424581006,\n        \"hamming_loss\": 0.24022346368715083,\n        \"avgPreScore\": 0.6285414749628938\n    }\n}",
        "accuracy": "0.7865",
        "precision": "0.7738",
        "recall": "0.7667",
        "f1": "0.7698",
        "holdout": "7.6947",
        "cv_score": "8.6585",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7865168539325843
    },
    {
        "id": "227",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:30.006169",
        "title": "AdaBoost Classifier",
        "description": "Sklearn AdaBoost Classifier",
        "model_type": "ensemble",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.MEstimateEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.GLMMEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", AdaBoostClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    Normalizer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.MEstimateEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), PowerTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.GLMMEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", AdaBoostClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "11",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7851123595505618,\n        \"precision\": 0.7711066707400649,\n        \"recall\": 0.7759345166061584,\n        \"f1\": 0.7732271662763466,\n        \"balancedAccuracyScore\": 0.7759345166061584,\n        \"auc_roc\": 0.7759345166061584,\n        \"log_loss\": 7.745335629964781,\n        \"jaccard_score\": 0.5641025641025641,\n        \"hinge_loss\": 0.8384831460674157,\n        \"hamming_loss\": 0.2148876404494382,\n        \"avgPreScore\": 0.6188967494090186\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7821229050279329,\n        \"precision\": 0.7921554252199414,\n        \"recall\": 0.7564350064350065,\n        \"f1\": 0.7636843708743781,\n        \"balancedAccuracyScore\": 0.7564350064350065,\n        \"auc_roc\": 0.7564350064350065,\n        \"log_loss\": 7.853086492600944,\n        \"jaccard_score\": 0.5357142857142857,\n        \"hinge_loss\": 0.8044692737430168,\n        \"hamming_loss\": 0.21787709497206703,\n        \"avgPreScore\": 0.6595541707273551\n    }\n}",
        "accuracy": "0.7851",
        "precision": "0.7711",
        "recall": "0.7759",
        "f1": "0.7732",
        "holdout": "7.7453",
        "cv_score": "7.8531",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7851123595505618
    },
    {
        "id": "229",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.486730",
        "title": "Linear Support Vector Machines",
        "description": "Sklearn Linear Support Vector Machines (SVM) Classifier",
        "model_type": "hyperplane",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.JamesSteinEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-999)),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), Normalizer()),\n\t)),\n\t(\"clf\", LinearSVC())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.JamesSteinEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), RobustScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), Normalizer()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]),\n                    OrdinalEncoder(\n                        handle_unknown=\"use_encoded_value\", unknown_value=-999\n                    ),\n                ),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), Normalizer()),\n            ),\n        ),\n        (\"clf\", LinearSVC()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "7",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7823033707865169,\n        \"precision\": 0.7695013314597483,\n        \"recall\": 0.7611099905876025,\n        \"f1\": 0.7646501847867115,\n        \"balancedAccuracyScore\": 0.7611099905876025,\n        \"auc_roc\": 0.7611099905876025,\n        \"log_loss\": 7.846581847349943,\n        \"jaccard_score\": 0.5386904761904762,\n        \"hinge_loss\": 0.8412921348314607,\n        \"hamming_loss\": 0.21769662921348315,\n        \"avgPreScore\": 0.613124896028858\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7877094972067039,\n        \"precision\": 0.7845937248592116,\n        \"recall\": 0.7731660231660231,\n        \"f1\": 0.777129750982962,\n        \"balancedAccuracyScore\": 0.7731660231660231,\n        \"auc_roc\": 0.7731660231660231,\n        \"log_loss\": 7.651725300482971,\n        \"jaccard_score\": 0.5730337078651685,\n        \"hinge_loss\": 0.7988826815642458,\n        \"hamming_loss\": 0.2122905027932961,\n        \"avgPreScore\": 0.6610469026670144\n    }\n}",
        "accuracy": "0.7823",
        "precision": "0.7695",
        "recall": "0.7611",
        "f1": "0.7647",
        "holdout": "7.8466",
        "cv_score": "7.6517",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7823033707865169
    },
    {
        "id": "233",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.589481",
        "title": "Logistic Regression",
        "description": "Sklearn Logistic Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.LeaveOneOutEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t)),\n\t(\"clf\", LogisticRegression())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.LeaveOneOutEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n            ),\n        ),\n        (\"clf\", LogisticRegression()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7823033707865169,\n        \"precision\": 0.7716802711985369,\n        \"recall\": 0.7559331719779481,\n        \"f1\": 0.7617767876994277,\n        \"balancedAccuracyScore\": 0.7559331719779481,\n        \"auc_roc\": 0.7559331719779481,\n        \"log_loss\": 7.846581847349943,\n        \"jaccard_score\": 0.5288753799392097,\n        \"hinge_loss\": 0.8412921348314607,\n        \"hamming_loss\": 0.21769662921348315,\n        \"avgPreScore\": 0.6127465112876924\n    },\n    \"holdout\": {\n        \"accuracy\": 0.770949720670391,\n        \"precision\": 0.7758843830888698,\n        \"recall\": 0.7469111969111969,\n        \"f1\": 0.7532030803376265,\n        \"balancedAccuracyScore\": 0.7469111969111969,\n        \"auc_roc\": 0.7469111969111969,\n        \"log_loss\": 8.25580887683689,\n        \"jaccard_score\": 0.5232558139534884,\n        \"hinge_loss\": 0.8156424581005587,\n        \"hamming_loss\": 0.22905027932960895,\n        \"avgPreScore\": 0.6420965216907587\n    }\n}",
        "accuracy": "0.7823",
        "precision": "0.7717",
        "recall": "0.7559",
        "f1": "0.7618",
        "holdout": "7.8466",
        "cv_score": "8.2558",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7823033707865169
    },
    {
        "id": "226",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.551476",
        "title": "Decision Tree Classifier",
        "description": "Sklearn Decision Tree Classifier",
        "model_type": "tree",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.HelmertEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.SumEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PowerTransformer()),\n\t)),\n\t(\"clf\", tree.DecisionTreeClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    StandardScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.HelmertEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), MinMaxScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.SumEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), PowerTransformer()),\n            ),\n        ),\n        (\"clf\", tree.DecisionTreeClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.7247191011235955,\n        \"precision\": 0.7069381598793363,\n        \"recall\": 0.7075433642597821,\n        \"f1\": 0.7072343853583626,\n        \"balancedAccuracyScore\": 0.7075433642597821,\n        \"auc_roc\": 0.7075433642597821,\n        \"log_loss\": 9.922129303745734,\n        \"jaccard_score\": 0.4659400544959128,\n        \"hinge_loss\": 0.898876404494382,\n        \"hamming_loss\": 0.2752808988764045,\n        \"avgPreScore\": 0.54034043266812\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7541899441340782,\n        \"precision\": 0.805524449421426,\n        \"recall\": 0.7106821106821107,\n        \"f1\": 0.7141405342624855,\n        \"balancedAccuracyScore\": 0.7106821106821107,\n        \"auc_roc\": 0.7106821106821106,\n        \"log_loss\": 8.85989245319081,\n        \"jaccard_score\": 0.4358974358974359,\n        \"hinge_loss\": 0.8324022346368715,\n        \"hamming_loss\": 0.24581005586592178,\n        \"avgPreScore\": 0.634558992982986\n    }\n}",
        "accuracy": "0.7247",
        "precision": "0.7069",
        "recall": "0.7075",
        "f1": "0.7072",
        "holdout": "9.9221",
        "cv_score": "8.8599",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.7247191011235955
    },
    {
        "id": "245",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.966110",
        "title": "Decision Tree Classifier",
        "description": "Sklearn Decision Tree Classifier",
        "model_type": "tree",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.GLMMEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.SumEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PolynomialFeatures(interaction_only=True)),\n\t)),\n\t(\"clf\", tree.DecisionTreeClassifier())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    Normalizer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.GLMMEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Parch\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), RobustScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.SumEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n            ),\n        ),\n        (\"clf\", tree.DecisionTreeClassifier()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "7",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6966292134831461,\n        \"precision\": 0.6762849872773538,\n        \"recall\": 0.6746672045179507,\n        \"f1\": 0.6754210459668228,\n        \"balancedAccuracyScore\": 0.6746672045179507,\n        \"auc_roc\": 0.6746672045179507,\n        \"log_loss\": 10.934591477597339,\n        \"jaccard_score\": 0.42091152815013405,\n        \"hinge_loss\": 0.9269662921348315,\n        \"hamming_loss\": 0.30337078651685395,\n        \"avgPreScore\": 0.5069442221946276\n    },\n    \"holdout\": {\n        \"accuracy\": 0.7653631284916201,\n        \"precision\": 0.7820930232558139,\n        \"recall\": 0.7341698841698842,\n        \"f1\": 0.7409015715467329,\n        \"balancedAccuracyScore\": 0.7341698841698842,\n        \"auc_roc\": 0.734169884169884,\n        \"log_loss\": 8.457170068954863,\n        \"jaccard_score\": 0.4939759036144578,\n        \"hinge_loss\": 0.8212290502793296,\n        \"hamming_loss\": 0.2346368715083799,\n        \"avgPreScore\": 0.6386818662237657\n    }\n}",
        "accuracy": "0.6966",
        "precision": "0.6763",
        "recall": "0.6747",
        "f1": "0.6754",
        "holdout": "10.9346",
        "cv_score": "8.4572",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6966292134831461
    },
    {
        "id": "238",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.746571",
        "title": "Quadratic Discriminant Analysis",
        "description": "Sklearn Quadratic Discriminant Analysis (QDA) Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.CatBoostEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.HelmertEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), MinMaxScaler()),\n\t)),\n\t(\"dense_transformer\", DenseTransformer()),(\"clf\", QuadraticDiscriminantAnalysis()),\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    StandardScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.CatBoostEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.HelmertEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), MinMaxScaler()),\n            ),\n        ),\n        (\"dense_transformer\", DenseTransformer()),\n        (\"clf\", QuadraticDiscriminantAnalysis()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6938202247191011,\n        \"precision\": 0.6715648785461961,\n        \"recall\": 0.6583635874680651,\n        \"f1\": 0.6621479256453789,\n        \"balancedAccuracyScore\": 0.6583635874680651,\n        \"auc_roc\": 0.6583635874680651,\n        \"log_loss\": 11.035837694982499,\n        \"jaccard_score\": 0.38764044943820225,\n        \"hinge_loss\": 0.9297752808988764,\n        \"hamming_loss\": 0.3061797752808989,\n        \"avgPreScore\": 0.4970077275945024\n    },\n    \"holdout\": {\n        \"accuracy\": 0.6033519553072626,\n        \"precision\": 0.6988505747126437,\n        \"recall\": 0.5222651222651222,\n        \"f1\": 0.42339276802322945,\n        \"balancedAccuracyScore\": 0.5222651222651222,\n        \"auc_roc\": 0.5222651222651222,\n        \"log_loss\": 14.296644640376078,\n        \"jaccard_score\": 0.05333333333333334,\n        \"hinge_loss\": 0.9832402234636871,\n        \"hamming_loss\": 0.39664804469273746,\n        \"avgPreScore\": 0.43430469575720976\n    }\n}",
        "accuracy": "0.6938",
        "precision": "0.6716",
        "recall": "0.6584",
        "f1": "0.6621",
        "holdout": "11.0358",
        "cv_score": "14.2966",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6938202247191011
    },
    {
        "id": "247",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.739521",
        "title": "ElasticNet",
        "description": "Sklearn ElasticNet Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.BaseNEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BinaryEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), FunctionTransformer(np.log1p)),\n\t)),\n\t(\"clf\", ElasticNet())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    MinMaxScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.BaseNEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), PowerTransformer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.BinaryEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]), FunctionTransformer(np.log1p)\n                ),\n            ),\n        ),\n        (\"clf\", ElasticNet()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "10",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6278089887640449,\n        \"precision\": 0.81311706629055,\n        \"recall\": 0.5055970149253731,\n        \"f1\": 0.3961525044565277,\n        \"balancedAccuracyScore\": 0.5055970149253731,\n        \"auc_roc\": 0.5055970149253731,\n        \"log_loss\": 13.415123803533772,\n        \"jaccard_score\": 0.011194029850746268,\n        \"hinge_loss\": 0.9957865168539326,\n        \"hamming_loss\": 0.37219101123595505,\n        \"avgPreScore\": 0.3833850410867013\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6278",
        "precision": "0.8131",
        "recall": "0.5056",
        "f1": "0.3962",
        "holdout": "13.4151",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6278089887640449
    },
    {
        "id": "232",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.490652",
        "title": "Lasso Regression",
        "description": "Sklearn Lasso Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), PowerTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.LeaveOneOutEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BackwardDifferenceEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), StandardScaler()),\n\t)),\n\t(\"clf\", Lasso())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    PowerTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.LeaveOneOutEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Parch\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), StandardScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]), ce.BackwardDifferenceEncoder()\n                ),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), StandardScaler()),\n            ),\n        ),\n        (\"clf\", Lasso()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6235955056179775,\n        \"precision\": 0.31179775280898875,\n        \"recall\": 0.5,\n        \"f1\": 0.3840830449826989,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 13.566993129611511,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.37640449438202245,\n        \"avgPreScore\": 0.37640449438202245\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6236",
        "precision": "0.3118",
        "recall": "0.5",
        "f1": "0.3841",
        "holdout": "13.567",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6235955056179775
    },
    {
        "id": "228",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.466581",
        "title": "ElasticNet",
        "description": "Sklearn ElasticNet Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.TargetEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.TargetEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), MinMaxScaler()),\n\t)),\n\t(\"clf\", ElasticNet())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    MinMaxScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.TargetEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.TargetEncoder()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), MinMaxScaler()),\n            ),\n        ),\n        (\"clf\", ElasticNet()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "7",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6235955056179775,\n        \"precision\": 0.31179775280898875,\n        \"recall\": 0.5,\n        \"f1\": 0.3840830449826989,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 13.566993129611511,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.37640449438202245,\n        \"avgPreScore\": 0.37640449438202245\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6236",
        "precision": "0.3118",
        "recall": "0.5",
        "f1": "0.3841",
        "holdout": "13.567",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6235955056179775
    },
    {
        "id": "240",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.458728",
        "title": "Lasso Regression",
        "description": "Sklearn Lasso Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.WOEEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.CountEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"clf\", Lasso())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    MinMaxScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.WOEEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), RobustScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), RobustScaler()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.CountEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"clf\", Lasso()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6235955056179775,\n        \"precision\": 0.31179775280898875,\n        \"recall\": 0.5,\n        \"f1\": 0.3840830449826989,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 13.566993129611511,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.37640449438202245,\n        \"avgPreScore\": 0.37640449438202245\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6236",
        "precision": "0.3118",
        "recall": "0.5",
        "f1": "0.3841",
        "holdout": "13.567",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6235955056179775
    },
    {
        "id": "251",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.387933",
        "title": "Lasso Regression",
        "description": "Sklearn Lasso Regression Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.CountEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), Normalizer()),\n\t)),\n\t(\"clf\", Lasso())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.linear_model import Lasso\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    FunctionTransformer(np.log1p),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.CountEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), MinMaxScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"SibSp\"]), Normalizer()),\n            ),\n        ),\n        (\"clf\", Lasso()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "6",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6235955056179775,\n        \"precision\": 0.31179775280898875,\n        \"recall\": 0.5,\n        \"f1\": 0.3840830449826989,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 13.566993129611511,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.37640449438202245,\n        \"avgPreScore\": 0.37640449438202245\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6236",
        "precision": "0.3118",
        "recall": "0.5",
        "f1": "0.3841",
        "holdout": "13.567",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6235955056179775
    },
    {
        "id": "255",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.761818",
        "title": "Bagging using Decision Tree Classifier",
        "description": "Sklearn Bagging using Decision Tree Classifier",
        "model_type": "tree",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.LeaveOneOutEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.BaseNEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PolynomialFeatures(interaction_only=True)),\n\t)),\n\t(\"clf\", BaggingClassifier(base_estimator=base_classifier))\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.LeaveOneOutEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), Normalizer()),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.BaseNEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n            ),\n        ),\n        (\"clf\", BaggingClassifier(base_estimator=base_classifier)),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.6193820224719101,\n        \"precision\": 0.5633598548070846,\n        \"recall\": 0.5328593518892026,\n        \"f1\": 0.5043553089559332,\n        \"balancedAccuracyScore\": 0.5328593518892026,\n        \"auc_roc\": 0.5328593518892026,\n        \"log_loss\": 13.718862455689253,\n        \"jaccard_score\": 0.153125,\n        \"hinge_loss\": 1.0042134831460674,\n        \"hamming_loss\": 0.3806179775280899,\n        \"avgPreScore\": 0.39628679663203614\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.6194",
        "precision": "0.5634",
        "recall": "0.5329",
        "f1": "0.5044",
        "holdout": "13.7189",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.6193820224719101
    },
    {
        "id": "236",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:29.914482",
        "title": "Quadratic Discriminant Analysis",
        "description": "Sklearn Quadratic Discriminant Analysis (QDA) Classifier",
        "model_type": "linear",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='median'), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['Cabin']), FillMissingCatWithUnknown(), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.BinaryEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), MinMaxScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), ce.CatBoostEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='uniform')),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"dense_transformer\", DenseTransformer()),(\"clf\", QuadraticDiscriminantAnalysis()),\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import KBinsDiscretizer\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"median\"),\n                    Normalizer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Cabin\"]),\n                    FillMissingCatWithUnknown(),\n                    TfidfTransformer(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.BinaryEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"PassengerId\"]), MinMaxScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Sex\"]), ce.CatBoostEncoder()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    KBinsDiscretizer(n_bins=3, encode=\"ordinal\", strategy=\"uniform\"),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"dense_transformer\", DenseTransformer()),\n        (\"clf\", QuadraticDiscriminantAnalysis()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "11",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.5688202247191011,\n        \"precision\": 0.528998028998029,\n        \"recall\": 0.5270774505849133,\n        \"f1\": 0.5264388235931322,\n        \"balancedAccuracyScore\": 0.5270774505849133,\n        \"auc_roc\": 0.5270774505849132,\n        \"log_loss\": 15.541294368622141,\n        \"jaccard_score\": 0.23821339950372208,\n        \"hinge_loss\": 1.0547752808988764,\n        \"hamming_loss\": 0.4311797752808989,\n        \"avgPreScore\": 0.39043909302168567\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5865921787709497,\n        \"precision\": 0.29329608938547486,\n        \"recall\": 0.5,\n        \"f1\": 0.36971830985915494,\n        \"balancedAccuracyScore\": 0.5,\n        \"auc_roc\": 0.5,\n        \"log_loss\": 14.90072821673,\n        \"jaccard_score\": 0.0,\n        \"hinge_loss\": 1.0,\n        \"hamming_loss\": 0.4134078212290503,\n        \"avgPreScore\": 0.4134078212290503\n    }\n}",
        "accuracy": "0.5688",
        "precision": "0.529",
        "recall": "0.5271",
        "f1": "0.5264",
        "holdout": "15.5413",
        "cv_score": "14.9007",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.5688202247191011
    },
    {
        "id": "239",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.560143",
        "title": "Gaussian Naive Bayes",
        "description": "Sklearn Gaussian Naive Bayes Classifier",
        "model_type": "probability",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), RobustScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), FillMissingCatWithUnknown(), ce.BinaryEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), Normalizer()),\n\t\tmake_pipeline(DataFrameSelector(['PassengerId']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), QuantileTransformer(n_quantiles=10, random_state=0)),\n\t)),\n\t(\"dense_transformer\", DenseTransformer()),(\"clf\", GaussianNB())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import OrdinalEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    RobustScaler(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    FillMissingCatWithUnknown(),\n                    ce.BinaryEncoder(),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Fare\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(DataFrameSelector([\"Parch\"]), Normalizer()),\n                make_pipeline(\n                    DataFrameSelector([\"PassengerId\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Pclass\"]), StandardScaler()),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    QuantileTransformer(n_quantiles=10, random_state=0),\n                ),\n            ),\n        ),\n        (\"dense_transformer\", DenseTransformer()),\n        (\"clf\", GaussianNB()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "8",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.5140449438202247,\n        \"precision\": 0.5557922077922077,\n        \"recall\": 0.5541549011698266,\n        \"f1\": 0.5137341597252378,\n        \"balancedAccuracyScore\": 0.5541549011698266,\n        \"auc_roc\": 0.5541549011698265,\n        \"log_loss\": 17.515595607632772,\n        \"jaccard_score\": 0.35687732342007433,\n        \"hinge_loss\": 1.1095505617977528,\n        \"hamming_loss\": 0.4859550561797753,\n        \"avgPreScore\": 0.40447369166134894\n    },\n    \"holdout\": {\n        \"accuracy\": 0.5921787709497207,\n        \"precision\": 0.5880475594493116,\n        \"recall\": 0.5905405405405405,\n        \"f1\": 0.5870231661451913,\n        \"balancedAccuracyScore\": 0.5905405405405405,\n        \"auc_roc\": 0.5905405405405405,\n        \"log_loss\": 14.699367024612021,\n        \"jaccard_score\": 0.3706896551724138,\n        \"hinge_loss\": 0.994413407821229,\n        \"hamming_loss\": 0.40782122905027934,\n        \"avgPreScore\": 0.4671430220887992\n    }\n}",
        "accuracy": "0.514",
        "precision": "0.5558",
        "recall": "0.5542",
        "f1": "0.5137",
        "holdout": "17.5156",
        "cv_score": "14.6994",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.5140449438202247
    },
    {
        "id": "237",
        "version": "001.001",
        "last_modified": "2023-01-16 07:04:28.559710",
        "title": "Gaussian Naive Bayes",
        "description": "Sklearn Gaussian Naive Bayes Classifier",
        "model_type": "probability",
        "generatedCode": "full_pipeline = Pipeline(steps=[\n\t(\"features\", make_union(\n\t\tmake_pipeline(DataFrameSelector(['Age']), SimpleImputer(missing_values=np.nan, strategy='mean'), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Embarked']), SimpleImputer(missing_values=np.nan, strategy='most_frequent'), ce.CountEncoder()),\n\t\tmake_pipeline(DataFrameSelector(['Fare']), StandardScaler()),\n\t\tmake_pipeline(DataFrameSelector(['Name']), TfidfTransformer()),\n\t\tmake_pipeline(DataFrameSelector(['Parch']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Pclass']), FunctionTransformer(np.log1p)),\n\t\tmake_pipeline(DataFrameSelector(['Sex']), OneHotEncoder(handle_unknown='ignore')),\n\t\tmake_pipeline(DataFrameSelector(['SibSp']), PolynomialFeatures(interaction_only=True)),\n\t\tmake_pipeline(DataFrameSelector(['Ticket']), TfidfTransformer()),\n\t)),\n\t(\"dense_transformer\", DenseTransformer()),(\"clf\", GaussianNB())\n])",
        "cleanedCode": "import pandas as pd\nimport numpy as np\nfrom sklearn.pipeline import Pipeline, make_pipeline, make_union\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nimport time\n\nobj = minioClient.get_object(\n    \"db67279b59ae8bd\",\n    \"titanic_new.csv\",\n)\n\n# Open Data\nmissing_values = [\"NaN\", \"n/a\", \"na\", \"--\", \"?\", \"\"]\ndataset = pd.read_csv(obj, na_values=missing_values, on_bad_lines=\"skip\")\n\nbase_classifier = DecisionTreeClassifier()\n\n# A class to select numerical or categorical columns\nclass DataFrameSelector(BaseEstimator, TransformerMixin):\n    def __init__(self, attribute_names):\n        self.attribute_names = attribute_names\n\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        return X[self.attribute_names]\n\n\ntarget_pipeline = Pipeline(\n    steps=[(\"selector\", DataFrameSelector([\"Survived\"])), (\"scaler\", OrdinalEncoder())]\n)\ntarget_pipelinef = target_pipeline.fit(dataset)\n\n# Transform data using the pipeline\ny = target_pipelinef.transform(dataset)\ny = pd.DataFrame(y, columns=[\"Survived\"])\ny = y.astype(\"int\").squeeze()\n\n\nX = dataset.drop(\"Survived\", axis=1)\n\n\nclass DenseTransformer(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X):\n        if type(X).__name__ != \"ndarray\":\n            return X.toarray()\n        else:\n            return X\n\n\nclass FillMissingCatWithUnknown(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        return self\n\n    def transform(self, X, y=None):\n        return X.fillna(\"Unknown\")\n\n\nclass TfidfTransformer(BaseEstimator, TransformerMixin):\n    def __init__(\n        self,\n        input=\"content\",\n        encoding=\"utf-8\",\n        decode_error=\"strict\",\n        strip_accents=None,\n        lowercase=True,\n        preprocessor=None,\n        tokenizer=None,\n        analyzer=\"word\",\n        stop_words=None,\n        token_pattern=r\"(?u)\b\\w\\w+\b\",\n        ngram_range=(1, 1),\n        max_df=1.0,\n        min_df=1,\n        max_features=None,\n        vocabulary=None,\n        binary=False,\n        dtype=np.float64,\n        norm=\"l2\",\n        use_idf=True,\n        smooth_idf=True,\n        sublinear_tf=False,\n    ):\n        self.input = input\n        self.encoding = encoding\n        self.decode_error = decode_error\n        self.strip_accents = strip_accents\n        self.preprocessor = preprocessor\n        self.tokenizer = tokenizer\n        self.analyzer = analyzer\n        self.lowercase = lowercase\n        self.token_pattern = token_pattern\n        self.stop_words = stop_words\n        self.ngram_range = ngram_range\n        self.max_df = max_df\n        self.min_df = min_df\n        self.max_features = max_features\n        self.binary = binary\n        self.dtype = dtype\n        self.norm = norm\n        self.use_idf = use_idf\n        self.smooth_idf = smooth_idf\n        self.sublinear_tf = sublinear_tf\n        self.tfidf_vectorizer = TfidfVectorizer(\n            decode_error=self.decode_error,\n            strip_accents=self.strip_accents,\n            lowercase=self.lowercase,\n            preprocessor=self.preprocessor,\n            tokenizer=self.tokenizer,\n            stop_words=self.stop_words,\n        )\n\n    def fit(self, X, y=None):\n        self.tfidf_vectorizer.fit(X.astype(str).squeeze())\n        return self\n\n    def transform(self, X):\n        return self.tfidf_vectorizer.transform(X.astype(str).squeeze())\n\n\nparam_grid = [\n    {\n        \"clf__bootstrap\": [True],\n        \"clf__max_depth\": [110],\n        \"clf__max_features\": [3],\n        \"clf__min_samples_leaf\": [5],\n        \"clf__min_samples_split\": [8, 12],\n        \"clf__n_estimators\": [100, 1000],\n    },\n]\n\nfull_pipeline = Pipeline(\n    steps=[\n        (\n            \"features\",\n            make_union(\n                make_pipeline(\n                    DataFrameSelector([\"Age\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"mean\"),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Embarked\"]),\n                    SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\"),\n                    ce.CountEncoder(),\n                ),\n                make_pipeline(DataFrameSelector([\"Fare\"]), StandardScaler()),\n                make_pipeline(DataFrameSelector([\"Name\"]), TfidfTransformer()),\n                make_pipeline(\n                    DataFrameSelector([\"Parch\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Pclass\"]), FunctionTransformer(np.log1p)\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"Sex\"]), OneHotEncoder(handle_unknown=\"ignore\")\n                ),\n                make_pipeline(\n                    DataFrameSelector([\"SibSp\"]),\n                    PolynomialFeatures(interaction_only=True),\n                ),\n                make_pipeline(DataFrameSelector([\"Ticket\"]), TfidfTransformer()),\n            ),\n        ),\n        (\"dense_transformer\", DenseTransformer()),\n        (\"clf\", GaussianNB()),\n    ]\n)\n\nX_train_h, X_test_holdout, y_train_h, y_test_holdout = train_test_split(\n    X, y, test_size=0.2, random_state=42\n)\ny_valid_pred = 0 * y_train_h\ny_valid_pred = y_valid_pred.squeeze()\n\ny_test_pred = 0\nreport = {}\nfolds = []\n\nK = 5\nkf = KFold(n_splits=K, random_state=1, shuffle=True)\nnp.random.seed(0)\n\nstart = time.time()\n\nfor i, (train_index, test_index) in enumerate(kf.split(X_train_h)):\n    y_train, y_valid = (\n        y_train_h.iloc[train_index].copy(),\n        y_train_h.iloc[test_index],\n    )\n    X_train, X_valid = (\n        X_train_h.iloc[train_index, :].copy(),\n        X_train_h.iloc[test_index, :].copy(),\n    )\n\n    fit_model = full_pipeline.fit(X_train, y_train)\n\n    # Generate validation predictions for this fold\n    pred = fit_model.predict(X_valid)\n\n    # Calculate Metrics per fold\n    fold_report = statIQ_Report(y_valid, pred)\n\n    folds.append(fold_report)\n\n    # Aggregate predictions per fold\n    y_valid_pred.iloc[test_index] = pred\n\n    # Accumulate Holdout set predictions\n    y_test_pred += fit_model.predict(X_test_holdout)\n\n    del X_train, X_valid, y_train\n\nend = time.time()\nreport[\"cv\"] = statIQ_Report(y_train_h, y_valid_pred)\n\n\n# Calculate Metrics for Holdout\ny_test_pred = y_test_pred / 5  # Average test set predictions\nreport[\"holdout\"] = statIQ_Report(y_test_holdout, y_test_pred.astype(\"int64\"))\n",
        "featureList": "None",
        "featureCount": "9",
        "cv_scores": "{\n    \"cv\": {\n        \"accuracy\": 0.5042134831460674,\n        \"precision\": 0.5473539308657082,\n        \"recall\": 0.5455324727712787,\n        \"f1\": 0.5036955856362865,\n        \"balancedAccuracyScore\": 0.5455324727712787,\n        \"auc_roc\": 0.5455324727712788,\n        \"log_loss\": 17.869957368480833,\n        \"jaccard_score\": 0.35110294117647056,\n        \"hinge_loss\": 1.1193820224719102,\n        \"hamming_loss\": 0.4957865168539326,\n        \"avgPreScore\": 0.3996302950995808\n    },\n    \"holdout\": {\n        \"accuracy\": 0.553072625698324,\n        \"precision\": 0.561854371378181,\n        \"recall\": 0.5631917631917631,\n        \"f1\": 0.5523880970242561,\n        \"balancedAccuracyScore\": 0.5631917631917631,\n        \"auc_roc\": 0.5631917631917631,\n        \"log_loss\": 16.108895369437832,\n        \"jaccard_score\": 0.36507936507936506,\n        \"hinge_loss\": 1.0335195530726258,\n        \"hamming_loss\": 0.44692737430167595,\n        \"avgPreScore\": 0.4482061585014498\n    }\n}",
        "accuracy": "0.5042",
        "precision": "0.5474",
        "recall": "0.5455",
        "f1": "0.5037",
        "holdout": "17.87",
        "cv_score": "16.1089",
        "status": "None",
        "job_id": "None",
        "projectHash": "db67279b59ae8bd",
        "project_id": "8",
        "accuracyorder": 0.5042134831460674
    }
]